# -*- coding: utf-8 -*-
# æ–‡ä»¶å: test_rllib_migration.py
# æè¿°: æµ‹è¯•RLlibè¿ç§»çš„åŸºæœ¬åŠŸèƒ½

import os
import sys
import numpy as np
import time

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from entities import UAV, Target
from scenarios import get_simple_scenario
from config import Config
from rllib_env import UAVTaskEnvRLlib

def test_environment_creation():
    """æµ‹è¯•ç¯å¢ƒåˆ›å»º"""
    print("=== æµ‹è¯•ç¯å¢ƒåˆ›å»º ===")
    
    # åˆ›å»ºç®€å•åœºæ™¯
    uavs, targets, obstacles = get_simple_scenario(50.0)
    
    # åˆ›å»ºæœ‰å‘å›¾
    from main import DirectedGraph
    graph = DirectedGraph(uavs, targets, 6, obstacles)
    
    # åˆ›å»ºé…ç½®
    config = Config()
    
    # åˆ›å»ºRLlibç¯å¢ƒ
    env = UAVTaskEnvRLlib(uavs, targets, graph, obstacles, config)
    
    print(f"âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    print(f"  è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"  åŠ¨ä½œç©ºé—´: {env.action_space}")
    print(f"  çŠ¶æ€ç»´åº¦: {env.state_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {env.action_dim}")
    
    return env

def test_environment_reset():
    """æµ‹è¯•ç¯å¢ƒé‡ç½®"""
    print("\n=== æµ‹è¯•ç¯å¢ƒé‡ç½® ===")
    
    env = test_environment_creation()
    
    # é‡ç½®ç¯å¢ƒ
    obs, info = env.reset()
    
    print(f"âœ“ ç¯å¢ƒé‡ç½®æˆåŠŸ")
    print(f"  è§‚å¯Ÿå½¢çŠ¶: {obs.shape}")
    print(f"  è§‚å¯Ÿç±»å‹: {type(obs)}")
    print(f"  ä¿¡æ¯: {info}")
    
    return env, obs

def test_environment_step():
    """æµ‹è¯•ç¯å¢ƒæ­¥è¿›"""
    print("\n=== æµ‹è¯•ç¯å¢ƒæ­¥è¿› ===")
    
    env, obs = test_environment_reset()
    
    # æµ‹è¯•å‡ ä¸ªéšæœºåŠ¨ä½œ
    for i in range(5):
        # éšæœºé€‰æ‹©åŠ¨ä½œ
        action = np.random.randint(0, env.action_space.n)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_obs, reward, terminated, truncated, info = env.step(action)
        
        print(f"  æ­¥éª¤ {i+1}:")
        print(f"    åŠ¨ä½œ: {action}")
        print(f"    å¥–åŠ±: {reward:.2f}")
        print(f"    ç»ˆæ­¢: {terminated}")
        print(f"    æˆªæ–­: {truncated}")
        print(f"    ä¿¡æ¯: {info}")
        
        if terminated:
            print("    ç¯å¢ƒå·²ç»ˆæ­¢")
            break
        
        obs = next_obs
    
    return env

def test_task_assignments():
    """æµ‹è¯•ä»»åŠ¡åˆ†é…"""
    print("\n=== æµ‹è¯•ä»»åŠ¡åˆ†é… ===")
    
    env = test_environment_step()
    
    # è·å–ä»»åŠ¡åˆ†é…
    assignments = env.get_task_assignments()
    
    print(f"âœ“ ä»»åŠ¡åˆ†é…è·å–æˆåŠŸ")
    print(f"  åˆ†é…ç»“æœ: {assignments}")
    
    # ç»Ÿè®¡åˆ†é…æƒ…å†µ
    total_assignments = sum(len(assignments[uav_id]) for uav_id in assignments)
    print(f"  æ€»åˆ†é…æ•°: {total_assignments}")
    
    return assignments

def test_reward_calculation():
    """æµ‹è¯•å¥–åŠ±è®¡ç®—"""
    print("\n=== æµ‹è¯•å¥–åŠ±è®¡ç®— ===")
    
    # åˆ›å»ºç¯å¢ƒ
    uavs, targets, obstacles = get_simple_scenario(50.0)
    from main import DirectedGraph
    graph = DirectedGraph(uavs, targets, 6, obstacles)
    config = Config()
    env = UAVTaskEnvRLlib(uavs, targets, graph, obstacles, config)
    
    # é‡ç½®ç¯å¢ƒ
    obs, info = env.reset()
    
    # æ‰§è¡Œä¸€äº›åŠ¨ä½œå¹¶è§‚å¯Ÿå¥–åŠ±
    rewards = []
    for i in range(10):
        action = np.random.randint(0, env.action_space.n)
        obs, reward, terminated, truncated, info = env.step(action)
        rewards.append(reward)
        
        if terminated:
            break
    
    print(f"âœ“ å¥–åŠ±è®¡ç®—æµ‹è¯•å®Œæˆ")
    print(f"  å¥–åŠ±åˆ—è¡¨: {[f'{r:.2f}' for r in rewards]}")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}")
    print(f"  å¥–åŠ±èŒƒå›´: [{min(rewards):.2f}, {max(rewards):.2f}]")

def test_action_mapping():
    """æµ‹è¯•åŠ¨ä½œæ˜ å°„"""
    print("\n=== æµ‹è¯•åŠ¨ä½œæ˜ å°„ ===")
    
    env = test_environment_creation()
    
    # æµ‹è¯•åŠ¨ä½œæ˜ å°„
    print(f"  åŠ¨ä½œç©ºé—´å¤§å°: {env.action_space.n}")
    print(f"  åŠ¨ä½œæ˜ å°„ç¤ºä¾‹:")
    
    for i in range(min(5, env.action_space.n)):
        action_tuple = env.action_to_tuple[i]
        action_idx = env.tuple_to_action[action_tuple]
        print(f"    ç´¢å¼• {i} -> å…ƒç»„ {action_tuple} -> ç´¢å¼• {action_idx}")
    
    print(f"âœ“ åŠ¨ä½œæ˜ å°„æµ‹è¯•å®Œæˆ")

def test_state_representation():
    """æµ‹è¯•çŠ¶æ€è¡¨ç¤º"""
    print("\n=== æµ‹è¯•çŠ¶æ€è¡¨ç¤º ===")
    
    env = test_environment_creation()
    obs, info = env.reset()
    
    print(f"âœ“ çŠ¶æ€è¡¨ç¤ºæµ‹è¯•å®Œæˆ")
    print(f"  çŠ¶æ€ç»´åº¦: {obs.shape}")
    print(f"  çŠ¶æ€ç±»å‹: {obs.dtype}")
    print(f"  çŠ¶æ€èŒƒå›´: [{obs.min():.2f}, {obs.max():.2f}]")
    print(f"  çŠ¶æ€å‡å€¼: {obs.mean():.2f}")
    print(f"  çŠ¶æ€æ ‡å‡†å·®: {obs.std():.2f}")

def run_comprehensive_test():
    """è¿è¡Œç»¼åˆæµ‹è¯•"""
    print("=== RLlibè¿ç§»ç»¼åˆæµ‹è¯• ===")
    print("å¼€å§‹æµ‹è¯•å„ä¸ªç»„ä»¶...")
    
    try:
        # æµ‹è¯•ç¯å¢ƒåˆ›å»º
        test_environment_creation()
        
        # æµ‹è¯•ç¯å¢ƒé‡ç½®
        test_environment_reset()
        
        # æµ‹è¯•ç¯å¢ƒæ­¥è¿›
        test_environment_step()
        
        # æµ‹è¯•ä»»åŠ¡åˆ†é…
        test_task_assignments()
        
        # æµ‹è¯•å¥–åŠ±è®¡ç®—
        test_reward_calculation()
        
        # æµ‹è¯•åŠ¨ä½œæ˜ å°„
        test_action_mapping()
        
        # æµ‹è¯•çŠ¶æ€è¡¨ç¤º
        test_state_representation()
        
        print("\n=== æ‰€æœ‰æµ‹è¯•é€šè¿‡! ===")
        print("âœ“ RLlibè¿ç§»æˆåŠŸ")
        print("âœ“ ç¯å¢ƒé€‚é…æ­£å¸¸")
        print("âœ“ åŸºæœ¬åŠŸèƒ½å®Œæ•´")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("RLlibè¿ç§»æµ‹è¯•å¼€å§‹...")
    
    success = run_comprehensive_test()
    
    if success:
        print("\nğŸ‰ è¿ç§»æµ‹è¯•æˆåŠŸå®Œæˆ!")
        print("ç°åœ¨å¯ä»¥è¿è¡ŒRLlibè®­ç»ƒäº†:")
        print("python main_rllib.py --scenario simple --episodes 100")
    else:
        print("\nğŸ’¥ è¿ç§»æµ‹è¯•å¤±è´¥!")
        print("è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜")

if __name__ == "__main__":
    main() 