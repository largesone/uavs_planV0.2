#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¯æŒä¸åŒç½‘ç»œç»“æ„çš„RLlibç¯å¢ƒ
é›†æˆGATç½‘ç»œå’Œæ·±åº¦æ®‹å·®ç½‘ç»œï¼Œç”¨äºå¯¹æ¯”ä¸åŒçš„ç»“æ„å½±å“
"""

import gymnasium as gym
import numpy as np
from gymnasium import spaces
from typing import Dict, List, Tuple, Any, Optional
import torch
import torch.nn as nn

from entities import UAV, Target
from scenarios import get_strategic_trap_scenario
from main import DirectedGraph, calculate_economic_sync_speeds
from config import Config
from rllib_networks import create_network, get_network_info

class UAVTaskEnvRLlibWithNetworks(gym.Env):
    """æ”¯æŒä¸åŒç½‘ç»œç»“æ„çš„UAVä»»åŠ¡ç¯å¢ƒ - RAYåº“ç‰ˆæœ¬"""
    
    def __init__(self, uavs: List[UAV], targets: List[Target], graph: DirectedGraph, 
                 obstacles: List, config: Config, network_type: str = 'SimpleFCN'):
        super(UAVTaskEnvRLlibWithNetworks, self).__init__()
        
        self.uavs = uavs
        self.targets = targets
        self.graph = graph
        self.obstacles = obstacles
        self.config = config
        self.network_type = network_type
        
        # è®¡ç®—çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç»´åº¦
        self._calculate_state_dim()
        self.n_actions = len(uavs) * len(targets) * self.graph.n_phi
        
        # è®¾ç½®è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(self.state_dim,), dtype=np.float32
        )
        self.action_space = spaces.Discrete(self.n_actions)
        
        # åˆå§‹åŒ–ç½‘ç»œï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        self._init_network()
        
        # ç¯å¢ƒçŠ¶æ€
        self.step_count = 0
        self.max_steps = len(targets) * len(uavs) * 2  # æœ€å¤§æ­¥æ•°
        self.invalid_action_penalty = -75.0
        
        print(f"UAVTaskEnvRLlibWithNetworks åˆå§‹åŒ–å®Œæˆ")
        print(f"  ç½‘ç»œç±»å‹: {network_type}")
        print(f"  çŠ¶æ€ç»´åº¦: {self.state_dim}")
        print(f"  åŠ¨ä½œç©ºé—´: {self.n_actions}")
        print(f"  æ— äººæœºæ•°é‡: {len(uavs)}")
        print(f"  ç›®æ ‡æ•°é‡: {len(targets)}")
    
    def _calculate_state_dim(self):
        """è®¡ç®—çŠ¶æ€ç»´åº¦"""
        # ç›®æ ‡ä¿¡æ¯: ä½ç½®(2) + èµ„æºéœ€æ±‚(2) + ä»·å€¼(1) + å‰©ä½™èµ„æº(2) = 7
        target_info_dim = 7 * len(self.targets)
        
        # UAVä¿¡æ¯: ä½ç½®(2) + èˆªå‘(1) + èµ„æº(2) + æœ€å¤§è·ç¦»(1) + é€Ÿåº¦èŒƒå›´(2) = 8
        uav_info_dim = 8 * len(self.uavs)
        
        # ååŒä¿¡æ¯: å½“å‰ä»»åŠ¡åˆ†é…çŠ¶æ€
        coordination_dim = len(self.targets) * len(self.uavs)
        
        # å…¨å±€çŠ¶æ€ä¿¡æ¯
        global_state_dim = 10  # æ­¥æ•°ã€å®Œæˆç‡ç­‰
        
        self.state_dim = target_info_dim + uav_info_dim + coordination_dim + global_state_dim
    
    def _init_network(self):
        """åˆå§‹åŒ–ç½‘ç»œï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
        try:
            # åˆ›å»ºç½‘ç»œå®ä¾‹ç”¨äºå‚æ•°ç»Ÿè®¡
            hidden_dims = [256, 128, 64]
            self.network = create_network(
                self.network_type, 
                self.state_dim, 
                hidden_dims, 
                self.n_actions
            )
            
            # è·å–ç½‘ç»œä¿¡æ¯
            network_info = get_network_info(self.network)
            print(f"  ç½‘ç»œå‚æ•°: {network_info['total_parameters']:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {network_info['trainable_parameters']:,}")
            
        except Exception as e:
            print(f"  ç½‘ç»œåˆå§‹åŒ–å¤±è´¥: {e}")
            self.network = None
    
    def _get_state(self) -> np.ndarray:
        """è·å–å½“å‰çŠ¶æ€"""
        state = []
        
        # ç›®æ ‡ä¿¡æ¯
        for target in self.targets:
            target_state = [
                target.position[0], target.position[1],  # ä½ç½®
                target.resources[0], target.resources[1],  # èµ„æºéœ€æ±‚
                target.value,  # ä»·å€¼
                target.remaining_resources[0], target.remaining_resources[1]  # å‰©ä½™èµ„æº
            ]
            state.extend(target_state)
        
        # UAVä¿¡æ¯
        for uav in self.uavs:
            uav_state = [
                uav.current_position[0], uav.current_position[1],  # ä½ç½®
                uav.heading,  # èˆªå‘
                uav.resources[0], uav.resources[1],  # èµ„æº
                uav.max_distance,  # æœ€å¤§è·ç¦»
                uav.velocity_range[0], uav.velocity_range[1]  # é€Ÿåº¦èŒƒå›´
            ]
            state.extend(uav_state)
        
        # ååŒä¿¡æ¯ - ä»»åŠ¡åˆ†é…çŠ¶æ€
        for target in self.targets:
            for uav in self.uavs:
                # æ£€æŸ¥æ˜¯å¦å·²åˆ†é…
                is_assigned = any(
                    (uav.id, phi_idx) in target.allocated_uavs 
                    for phi_idx in range(self.graph.n_phi)
                )
                state.append(1.0 if is_assigned else 0.0)
        
        # å…¨å±€çŠ¶æ€ä¿¡æ¯
        total_targets = len(self.targets)
        completed_targets = sum(
            1 for target in self.targets 
            if np.all(target.remaining_resources <= 0)
        )
        completion_rate = completed_targets / total_targets if total_targets > 0 else 0.0
        
        global_state = [
            self.step_count,  # å½“å‰æ­¥æ•°
            completion_rate,  # å®Œæˆç‡
            len([u for u in self.uavs if np.any(u.resources > 0)]),  # å¯ç”¨UAVæ•°é‡
            sum(np.sum(target.remaining_resources) for target in self.targets),  # æ€»å‰©ä½™éœ€æ±‚
            sum(np.sum(uav.resources) for uav in self.uavs),  # æ€»å¯ç”¨èµ„æº
            completed_targets,  # å·²å®Œæˆç›®æ ‡æ•°
            total_targets,  # æ€»ç›®æ ‡æ•°
            self.max_steps - self.step_count,  # å‰©ä½™æ­¥æ•°
            np.mean([uav.heading for uav in self.uavs]),  # å¹³å‡èˆªå‘
            np.std([uav.heading for uav in self.uavs])  # èˆªå‘æ ‡å‡†å·®
        ]
        state.extend(global_state)
        
        return np.array(state, dtype=np.float32)
    
    def _action_to_assignment(self, action: int) -> Tuple[int, int, int]:
        """å°†åŠ¨ä½œç´¢å¼•è½¬æ¢ä¸ºä»»åŠ¡åˆ†é…"""
        n_uavs = len(self.uavs)
        n_targets = len(self.targets)
        n_phi = self.graph.n_phi
        
        target_idx = action // (n_uavs * n_phi)
        remaining = action % (n_uavs * n_phi)
        uav_idx = remaining // n_phi
        phi_idx = remaining % n_phi
        
        return target_idx, uav_idx, phi_idx
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None) -> Tuple[np.ndarray, Dict[str, Any]]:
        """é‡ç½®ç¯å¢ƒçŠ¶æ€"""
        super().reset(seed=seed)
        
        # é‡ç½®æ‰€æœ‰å®ä½“
        for uav in self.uavs:
            uav.reset()
        for target in self.targets:
            target.reset()
        
        # é‡ç½®ç¯å¢ƒçŠ¶æ€
        self.step_count = 0
        
        # è¿”å›åˆå§‹çŠ¶æ€å’Œä¿¡æ¯
        initial_state = self._get_state()
        info = {
            'episode': {'r': 0.0, 'l': 0},
            'total_reward': 0.0,
            'step_count': 0,
            'network_type': self.network_type
        }
        
        return initial_state, info
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œ"""
        self.step_count += 1
        
        # è½¬æ¢åŠ¨ä½œ
        target_idx, uav_idx, phi_idx = self._action_to_assignment(action)
        target = self.targets[target_idx]
        uav = self.uavs[uav_idx]
        
        # æ£€æŸ¥åŠ¨ä½œæœ‰æ•ˆæ€§
        if not self._is_valid_action(target, uav, phi_idx):
            penalty = self.invalid_action_penalty
            return self._get_state(), penalty, False, False, {
                'invalid_action': True,
                'reason': 'invalid_assignment'
            }
        
        # è®¡ç®—å®é™…è´¡çŒ®
        actual_contribution = np.minimum(uav.resources, target.remaining_resources)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…è´¡çŒ®
        if np.all(actual_contribution <= 0):
            return self._get_state(), self.invalid_action_penalty, False, False, {
                'invalid_action': True,
                'reason': 'no_contribution'
            }
        
        # è®°å½•ç›®æ ‡å®Œæˆå‰çš„çŠ¶æ€
        was_satisfied = np.all(target.remaining_resources <= 0)
        
        # è®¡ç®—è·¯å¾„é•¿åº¦
        path_len = np.linalg.norm(uav.current_position - target.position)
        travel_time = path_len / uav.velocity_range[1]
        
        # æ›´æ–°çŠ¶æ€
        uav.resources = uav.resources.astype(np.float64) - actual_contribution.astype(np.float64)
        target.remaining_resources = target.remaining_resources.astype(np.float64) - actual_contribution.astype(np.float64)
        
        if uav_idx not in {a[0] for a in target.allocated_uavs}:
            target.allocated_uavs.append((uav_idx, phi_idx))
        uav.task_sequence.append((target_idx, phi_idx))
        uav.current_position = target.position
        uav.heading = self.graph.phi_set[phi_idx]
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆæ‰€æœ‰ç›®æ ‡
        total_satisfied = sum(np.all(t.remaining_resources <= 0) for t in self.targets)
        total_targets = len(self.targets)
        done = bool(total_satisfied == total_targets)
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(target, uav, actual_contribution, path_len, 
                                      was_satisfied, travel_time, done)
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        truncated = self.step_count >= self.max_steps
        
        # æ„å»ºä¿¡æ¯å­—å…¸ï¼ˆç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯PythonåŸç”Ÿç±»å‹ï¼‰
        info = {
            'target_id': int(target_idx),
            'uav_id': int(uav_idx),
            'phi_idx': int(phi_idx),
            'actual_contribution': float(np.sum(actual_contribution)),
            'path_length': float(path_len),
            'travel_time': float(travel_time),
            'done': bool(done),
            'network_type': self.network_type
        }
        
        return self._get_state(), reward, done, truncated, info
    
    def _is_valid_action(self, target: Target, uav: UAV, phi_idx: int) -> bool:
        """æ£€æŸ¥åŠ¨ä½œæ˜¯å¦æœ‰æ•ˆ"""
        # æ£€æŸ¥UAVæ˜¯å¦æœ‰èµ„æº
        if np.all(uav.resources <= 0):
            return False
        
        # æ£€æŸ¥ç›®æ ‡æ˜¯å¦è¿˜éœ€è¦èµ„æº
        if np.all(target.remaining_resources <= 0):
            return False
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ†é…è¿‡
        if (uav.id, phi_idx) in target.allocated_uavs:
            return False
        
        return True
    
    def _calculate_reward(self, target: Target, uav: UAV, actual_contribution: np.ndarray,
                         path_len: float, was_satisfied: bool, travel_time: float, done: bool) -> float:
        """è®¡ç®—å¥–åŠ±"""
        # 1. ç›®æ ‡å®Œæˆå¥–åŠ±
        now_satisfied = np.all(target.remaining_resources <= 0)
        new_satisfied = int(now_satisfied and not was_satisfied)
        target_completion_reward = 1500 if new_satisfied else 0
        
        # 2. è¾¹é™…æ•ˆç”¨é€’å‡å¥–åŠ±
        target_initial_total = np.sum(target.resources)
        target_remaining_before = np.sum(target.remaining_resources + actual_contribution)
        target_remaining_after = np.sum(target.remaining_resources)
        completion_ratio_before = 1.0 - (target_remaining_before / target_initial_total)
        completion_ratio_after = 1.0 - (target_remaining_after / target_initial_total)
        completion_improvement = completion_ratio_after - completion_ratio_before
        
        # è¾¹é™…æ•ˆç”¨é€’å‡
        marginal_utility = completion_improvement * (1.0 - completion_ratio_before)
        marginal_reward = marginal_utility * 1000
        
        # 3. èµ„æºæ•ˆç‡å¥–åŠ±
        resource_efficiency = np.sum(actual_contribution) / np.sum(uav.resources + actual_contribution)
        efficiency_reward = resource_efficiency * 500
        
        # 4. è·ç¦»æƒ©ç½š
        distance_penalty = -path_len * 0.1
        
        # 5. æ—¶é—´æƒ©ç½š
        time_penalty = -travel_time * 10
        
        # 6. å®Œæˆå¥–åŠ±
        completion_reward = 1000 if done else 0
        
        # æ€»å¥–åŠ±
        total_reward = (target_completion_reward + marginal_reward + efficiency_reward + 
                       distance_penalty + time_penalty + completion_reward)
        
        return float(total_reward)


def create_uav_env_with_networks(network_type: str = 'SimpleFCN'):
    """åˆ›å»ºæŒ‡å®šç½‘ç»œç±»å‹çš„UAVç¯å¢ƒ"""
    def env_creator(config):
        uavs, targets, obstacles = get_strategic_trap_scenario(50.0)
        graph = DirectedGraph(uavs, targets, 6, obstacles)
        config_obj = Config()
        return UAVTaskEnvRLlibWithNetworks(uavs, targets, graph, obstacles, config_obj, network_type)
    
    return env_creator


def test_network_comparison():
    """æµ‹è¯•ä¸åŒç½‘ç»œç»“æ„çš„å¯¹æ¯”"""
    print("ğŸ§ª æµ‹è¯•ä¸åŒç½‘ç»œç»“æ„çš„å¯¹æ¯”...")
    
    network_types = ['SimpleFCN', 'DeepResidual', 'GAT']
    
    for network_type in network_types:
        print(f"\n--- æµ‹è¯•ç½‘ç»œç±»å‹: {network_type} ---")
        
        try:
            # åˆ›å»ºç¯å¢ƒ
            env = create_uav_env_with_networks(network_type)()
            
            # æµ‹è¯•é‡ç½®
            obs, info = env.reset()
            print(f"  çŠ¶æ€å½¢çŠ¶: {obs.shape}")
            print(f"  ç½‘ç»œç±»å‹: {info['network_type']}")
            
            # æµ‹è¯•å‡ æ­¥
            for step in range(3):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                
                print(f"  æ­¥éª¤ {step+1}: å¥–åŠ±={reward:.2f}, ç»ˆæ­¢={terminated}, æˆªæ–­={truncated}")
                
                if terminated or truncated:
                    break
            
            print(f"âœ… {network_type} ç½‘ç»œæµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âŒ {network_type} ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
    
    print("\nğŸ‰ ç½‘ç»œç»“æ„å¯¹æ¯”æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_network_comparison() 